{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from datetime import date\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import nbimporter\n",
    "import sys\n",
    "import numpy as np\n",
    "import traceback\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    daily = pd.read_csv(file)\n",
    "    if 'Date' in daily.columns:\n",
    "        daily['Date'] = pd.to_datetime(daily['Date'])\n",
    "    else:\n",
    "        daily['Date'] = pd.to_datetime(daily['Datetime'], utc=True).dt.tz_convert('US/Eastern')\n",
    "    daily = daily.sort_values(by='Date', ascending=True)\n",
    "    daily['Day'] = daily['Date'].dt.strftime('%m/%d/%Y')\n",
    "    daily['Time'] = daily['Date'].dt.strftime('%H:%M')\n",
    "    daily = daily.dropna().reset_index(drop=True)\n",
    "    return daily\n",
    "\n",
    "def load_yahoo_data(file):\n",
    "    daily = pd.read_csv(file)\n",
    "    if 'Date' in daily.columns:\n",
    "        daily['Date'] = pd.to_datetime(daily['Date'])\n",
    "    else:\n",
    "        daily['Date'] = pd.to_datetime(daily['Datetime'], utc=True).dt.tz_convert('US/Eastern')\n",
    "    daily = daily.sort_values(by='Date', ascending=True)\n",
    "    daily['Day'] = daily['Date'].dt.strftime('%Y-%m-%d')\n",
    "    daily['Time'] = daily['Date'].dt.strftime('%H:%M')\n",
    "    daily = daily.dropna().reset_index(drop=True)\n",
    "    return daily\n",
    "def load_NKE_data():\n",
    "    files = [\"/Users/partha/Downloads/0-DTE/Stocks/NKE-20Year.csv\",\n",
    "            \n",
    "            ]\n",
    "    df = pd.DataFrame()\n",
    "    for file in files:\n",
    "        print ('Loading file {}'.format(file))\n",
    "        df = df.append(load_data(file))\n",
    "    df = df.drop_duplicates().sort_values(by='Date', ascending=True)\n",
    "    return df\n",
    "\n",
    "def load_stock_data(files):\n",
    "    df = pd.DataFrame()\n",
    "    for file in files:\n",
    "        print ('Loading file {}'.format(file))\n",
    "        df = df.append(load_data(file))\n",
    "    df = df.drop_duplicates().sort_values(by='Date', ascending=True)\n",
    "    return df\n",
    "\n",
    "def load_stocks_data():\n",
    "    files = [\"/Users/partha/Downloads/0-DTE/stocks/GOOG-Dec-16.csv\",\n",
    "#              \"/Users/partha/Downloads/0-DTE/indexes/NFLX-Dec-16.csv\",\n",
    "#              \"/Users/partha/Downloads/0-DTE/indexes/GOOG-Dec-16.csv\"\n",
    "            ]\n",
    "    df = pd.DataFrame()\n",
    "    for file in files:\n",
    "        print ('Loading file {}'.format(file))\n",
    "        df = df.append(load_data(file))\n",
    "        df\n",
    "    df = df.drop_duplicates().sort_values(by='Date', ascending=True)\n",
    "    return df\n",
    "def load_all_5min_spx():\n",
    "    files = [ \n",
    "        \"/Users/partha/Downloads/0-DTE/indexes/SPX-Dec-13.csv\",\n",
    "        \"/Users/partha/Downloads/0-DTE/indexes/SPX-Dec-16.csv\",\n",
    "        \"/Users/partha/Downloads/0-DTE/indexes/SPX-Dec-12.csv\",\n",
    "        \"/Users/partha/Downloads/0-DTE/indexes/SPX-Dec-9.csv\",\n",
    "        \"/Users/partha/Downloads/0-DTE/indexes/SPX-Dec-8.csv\",\n",
    "            \"/Users/partha/Downloads/0-DTE/indexes/SPX-DEC-6.csv\",\n",
    "             \"/Users/partha/Downloads/0-DTE/indexes/SPX-DEC-7.csv\",\n",
    "            \"/Users/partha/Downloads/0-DTE/indexes/spx-oct-28-snapshot.csv\",\n",
    "        \"/Users/partha/Downloads/0-DTE/indexes/SPX-Dec-20.csv\",\n",
    "    \"/Users/partha/Downloads/0-DTE/indexes/SPX-Dec-19.csv\"]\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for file in files:\n",
    "        print ('Loading file {}'.format(file))\n",
    "        df = df.append(load_data(file))\n",
    "    df = df[df['Day'] > '08/31/2022']\n",
    "    return df.drop_duplicates().sort_values(by='Date', ascending=True)\n",
    "# df = load_all_5min_spx()\n",
    "# df.head()\n",
    "\n",
    "def load_5min_vix():\n",
    "    files = [ \n",
    "        \"/Users/partha/Downloads/0-DTE/indexes/VIX-Dec-9.csv\",\n",
    "        \"/Users/partha/Downloads/0-DTE/indexes/VIX-Nov-09.csv\",\n",
    "        \"/Users/partha/Downloads/0-DTE/indexes/VIX-Nov-09.csv\"\n",
    "    ]\n",
    "    df = pd.DataFrame()\n",
    "    for file in files:\n",
    "        print ('Loading file {}'.format(file))\n",
    "        df = df.append(load_data(file))\n",
    "    df = df[df['Day'] > '08/31/2022']\n",
    "    return df.drop_duplicates().sort_values(by='Date', ascending=True)\n",
    "    \n",
    "def load_annual_data():\n",
    "    files = [\"/Users/partha/Downloads/0-DTE/indexes/SPX-20Year.csv\",\n",
    "            \n",
    "            ]\n",
    "    df = pd.DataFrame()\n",
    "    for file in files:\n",
    "        print ('Loading file {}'.format(file))\n",
    "        df = df.append(load_data(file))\n",
    "    df = df.drop_duplicates().sort_values(by='Date', ascending=True)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_seq(segment, split_size=10):\n",
    "    \n",
    "    rge = segment['High'].max() - segment['Low'].min()\n",
    "    ilist = np.array_split(segment, split_size)\n",
    "    amap = []\n",
    "   \n",
    "    for idf in ilist:\n",
    "        try:\n",
    "            idf = idf.reset_index(drop=True)\n",
    "            s = int((idf[:1]['Open'].min()-segment['Low'].min())*split_size/rge)\n",
    "            e = int((idf.iloc[-1]['Close'].min()-segment['Low'].min())*split_size/rge)\n",
    "            if not amap:\n",
    "                amap.append(s)\n",
    "            else:    \n",
    "                amap.append(e)\n",
    "        except Exception as e:\n",
    "            print (\"Exception while building Sequence {}\".format(e))\n",
    "            pass    \n",
    "    return amap\n",
    "\n",
    "\n",
    "def predict(amap, seq_map, dates=None, times=None):\n",
    "    lowest = sys.maxsize\n",
    "    mystack = []\n",
    "    match_seq = None\n",
    "    for seq_key in seq_map.keys():\n",
    "        seq = seq_map[seq_key]\n",
    "        aseq = seq['Sequence']\n",
    "        \n",
    "        if aseq is None:\n",
    "            continue\n",
    "            \n",
    "        if dates is not None:\n",
    "            end_date = seq['Start Date']\n",
    "            if end_date in dates: # and seq['Start Time'] == times[dates.index(end_date)]:\n",
    "                continue\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            dist = np.linalg.norm(np.array(aseq)-np.array(amap))\n",
    "            if dist == 0:\n",
    "                continue\n",
    "                \n",
    "            if dist < lowest:\n",
    "                lowest = dist\n",
    "                match_seq = str(aseq)\n",
    "                mystack.append(seq)\n",
    "            \n",
    "        except Exception as e:\n",
    "            #print (e)\n",
    "            pass\n",
    "    prediction = seq_map[match_seq].copy() if seq else {}\n",
    "    try:\n",
    "        prediction['Distance'] = lowest\n",
    "        #prediction['stack'] = mystack\n",
    "        pass\n",
    "    except:\n",
    "        pass\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def test_a_segment(day, asegment, seqmap, split_size=20):\n",
    "    asegment = asegment.reset_index(drop=True)\n",
    "    #print ('Start date {} and End Date {}'.format(asegment.iloc[0]['Date'], asegment.iloc[-1]['Date']))\n",
    "    #print ('lenth {}'.format(len(asegment)))\n",
    "    if len(asegment) != split_size:\n",
    "        print (\"Input size and split size doesn't match {} / {}\".format(len(asegment), split_size))\n",
    "        return\n",
    "    \n",
    "    amap = build_seq(asegment, split_size)\n",
    "    #print ('running prediction with {} and {} '.format(amap, seqmap))\n",
    "    return predict(amap, seqmap)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_stock(sym, horizon=9):\n",
    "    df = load_yahoo_data('/Users/partha/experiments/candlestick-patterns/high-predictability-data/{}.csv'.format(sym))\n",
    "    test_df = df[-20:]\n",
    "    model = json.loads(open('/Users/partha/Downloads/0-DTE/Stocks/sp500-models-cleaned/{}.json'.format(sym)).read())\n",
    "    match = test_a_segment(date, test_df, model, 20)\n",
    "    pred_df = df[(df['Date'] >= match['End Date'])]\n",
    "    pred = 1 if pred_df.iloc[horizon]['Close'] > pred_df.iloc[0]['Close'] else -1\n",
    "    prediction = {'S': sym, 'D0': match['Start date'], 'D1': match['End Date'], 'Dist': match['Distance'], \"P\": pred}\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>D0</th>\n",
       "      <th>D1</th>\n",
       "      <th>Dist</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EL</td>\n",
       "      <td>2007-09-18</td>\n",
       "      <td>2007-10-15</td>\n",
       "      <td>6.855655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABMD</td>\n",
       "      <td>2014-10-17</td>\n",
       "      <td>2014-11-13</td>\n",
       "      <td>8.660254</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TFX</td>\n",
       "      <td>2013-09-12</td>\n",
       "      <td>2013-10-09</td>\n",
       "      <td>9.055385</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FE</td>\n",
       "      <td>2020-11-30</td>\n",
       "      <td>2020-12-28</td>\n",
       "      <td>9.433981</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PG</td>\n",
       "      <td>2019-11-26</td>\n",
       "      <td>2019-12-24</td>\n",
       "      <td>9.695360</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BEN</td>\n",
       "      <td>2018-04-11</td>\n",
       "      <td>2018-05-08</td>\n",
       "      <td>10.198039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YUM</td>\n",
       "      <td>2012-09-04</td>\n",
       "      <td>2012-10-01</td>\n",
       "      <td>12.165525</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ETR</td>\n",
       "      <td>2016-03-14</td>\n",
       "      <td>2016-04-11</td>\n",
       "      <td>12.369317</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CE</td>\n",
       "      <td>2005-08-08</td>\n",
       "      <td>2005-09-02</td>\n",
       "      <td>17.435596</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>2013-03-28</td>\n",
       "      <td>18.681542</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>VRTX</td>\n",
       "      <td>2008-10-29</td>\n",
       "      <td>2008-11-25</td>\n",
       "      <td>18.814888</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>JPM</td>\n",
       "      <td>2008-12-03</td>\n",
       "      <td>2008-12-31</td>\n",
       "      <td>20.174241</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DRI</td>\n",
       "      <td>2015-12-18</td>\n",
       "      <td>2016-01-19</td>\n",
       "      <td>22.605309</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVY</td>\n",
       "      <td>2020-07-14</td>\n",
       "      <td>2020-08-10</td>\n",
       "      <td>23.685439</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        S          D0          D1       Dist  P\n",
       "6      EL  2007-09-18  2007-10-15   6.855655  1\n",
       "1    ABMD  2014-10-17  2014-11-13   8.660254  1\n",
       "3     TFX  2013-09-12  2013-10-09   9.055385  1\n",
       "9      FE  2020-11-30  2020-12-28   9.433981  1\n",
       "2      PG  2019-11-26  2019-12-24   9.695360  1\n",
       "8     BEN  2018-04-11  2018-05-08  10.198039  1\n",
       "0     YUM  2012-09-04  2012-10-01  12.165525  1\n",
       "13    ETR  2016-03-14  2016-04-11  12.369317  1\n",
       "11     CE  2005-08-08  2005-09-02  17.435596  1\n",
       "10  GOOGL  2013-03-01  2013-03-28  18.681542  1\n",
       "12   VRTX  2008-10-29  2008-11-25  18.814888  1\n",
       "5     JPM  2008-12-03  2008-12-31  20.174241 -1\n",
       "7     DRI  2015-12-18  2016-01-19  22.605309 -1\n",
       "4     AVY  2020-07-14  2020-08-10  23.685439  1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "for sym in pd.read_csv('/Users/partha/experiments/candlestick-patterns/High-Predictability-Symbols.csv')['Symbol']:\n",
    "    predictions.append(predict_stock(sym))\n",
    "\n",
    "pd.DataFrame(predictions).sort_values(by='Dist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/partha/experiments/candlestick-patterns/high-predictability-data/ORLY.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-134037bad372>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict_stock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ORLY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-3331a2908262>\u001b[0m in \u001b[0;36mpredict_stock\u001b[0;34m(sym, horizon)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_stock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msym\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_yahoo_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/partha/experiments/candlestick-patterns/high-predictability-data/{}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msym\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/partha/Downloads/0-DTE/Stocks/sp500-models-cleaned/{}.json'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msym\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_a_segment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-aebd9c521330>\u001b[0m in \u001b[0;36mload_yahoo_data\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_yahoo_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdaily\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'Date'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdaily\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdaily\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdaily\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/partha/experiments/candlestick-patterns/high-predictability-data/ORLY.csv'"
     ]
    }
   ],
   "source": [
    "predict_stock('ORLY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
